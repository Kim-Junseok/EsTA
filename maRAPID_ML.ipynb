{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN approach for maRAPID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goodsol Lee, Netlab, Seoul National University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os\n",
    "\n",
    "#configuration for gpu usage\n",
    "conf = tf.ConfigProto()\n",
    "# you can modify below as you want\n",
    "#conf.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "#conf.gpu_options.allow_growth = True\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Sample training data, validation data, test data from raw data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get sample from node1\n",
      "25753\n",
      "Get sample from node2\n",
      "15583\n",
      "Get sample from node3\n",
      "25360\n",
      "Get sample from node4\n",
      "18582\n",
      "Get sample from node5\n",
      "26572\n",
      "Get sample from node6\n",
      "21183\n",
      "Get sample from node7\n",
      "30573\n",
      "Get sample from node8\n",
      "52965\n",
      "Get sample from node9\n",
      "26581\n",
      "Get sample from node10\n",
      "21779\n",
      "Get sample from node11\n",
      "21552\n",
      "Get sample from node12\n",
      "16762\n",
      "Get sample from node13\n",
      "33371\n",
      "Get sample from node14\n",
      "23378\n",
      "Get sample from node15\n",
      "52162\n",
      "Get sample from node16\n",
      "34773\n",
      "Get sample from node17\n",
      "26372\n",
      "Get sample from node18\n",
      "29170\n",
      "Get sample from node19\n",
      "22372\n",
      "Get sample from node20\n",
      "24572\n",
      "Get sample from node21\n",
      "41962\n",
      "Get sample from node22\n",
      "40942\n",
      "Get sample from node23\n",
      "11791\n",
      "Get sample from node24\n",
      "23972\n",
      "Get sample from node25\n",
      "27971\n",
      "Get sample from node26\n",
      "42366\n",
      "Get sample from node27\n",
      "29372\n",
      "Get sample from node28\n",
      "20782\n",
      "Get sample from node29\n",
      "24181\n",
      "Get sample from node30\n",
      "25184\n",
      "Get sample from node31\n",
      "35176\n",
      "Get sample from node32\n",
      "34172\n",
      "Get sample from node33\n",
      "25181\n",
      "Get sample from node34\n",
      "20571\n",
      "Get sample from node35\n",
      "17772\n",
      "Get sample from node36\n",
      "29172\n",
      "Get sample from node37\n",
      "22191\n",
      "Get sample from node38\n",
      "9390\n",
      "Get sample from node39\n",
      "24569\n",
      "Get sample from node40\n",
      "20582\n",
      "Get sample from node41\n",
      "17581\n",
      "Get sample from node42\n",
      "22981\n",
      "Get sample from node43\n",
      "15969\n",
      "Get sample from node44\n",
      "22772\n",
      "Get sample from node45\n",
      "35544\n",
      "Get sample from node46\n",
      "38174\n",
      "Get sample from node47\n",
      "27372\n",
      "Get sample from node48\n",
      "32372\n",
      "Get sample from node49\n",
      "20790\n",
      "Get sample from node50\n",
      "14381\n",
      "Get sample from node51\n",
      "41383\n",
      "Get sample from node52\n",
      "27952\n",
      "Get sample from node53\n",
      "14781\n",
      "Get sample from node54\n",
      "26371\n",
      "Get sample from node55\n",
      "27382\n",
      "Get sample from node56\n",
      "21781\n",
      "Get sample from node57\n",
      "41368\n",
      "Get sample from node58\n",
      "27174\n",
      "Get sample from node59\n",
      "23971\n",
      "Get sample from node60\n",
      "27952\n",
      "Get sample from node61\n",
      "23581\n",
      "Get sample from node62\n",
      "25782\n",
      "Get sample from node63\n",
      "34962\n",
      "Get sample from node64\n",
      "27372\n",
      "Get sample from node65\n",
      "19582\n",
      "Get sample from node66\n",
      "23382\n",
      "Get sample from node67\n",
      "39742\n",
      "Get sample from node68\n",
      "11392\n",
      "Get sample from node69\n",
      "20965\n",
      "Get sample from node70\n",
      "27571\n",
      "Get sample from node71\n",
      "25172\n",
      "Get sample from node72\n",
      "18572\n",
      "Get sample from node73\n",
      "22172\n",
      "Get sample from node74\n",
      "35962\n",
      "Get sample from node75\n",
      "30562\n",
      "Get sample from node76\n",
      "16181\n",
      "Get sample from node77\n",
      "23781\n",
      "Get sample from node78\n",
      "26162\n",
      "Get sample from node79\n",
      "43762\n",
      "Get sample from node80\n",
      "38172\n",
      "Get sample from node81\n",
      "46942\n",
      "Get sample from node82\n",
      "27981\n",
      "Get sample from node83\n",
      "20568\n",
      "Get sample from node84\n",
      "28987\n",
      "Get sample from node85\n",
      "44770\n",
      "Get sample from node86\n",
      "22581\n",
      "Get sample from node87\n",
      "41575\n",
      "Get sample from node88\n",
      "26969\n",
      "Get sample from node89\n",
      "22942\n",
      "Get sample from node90\n",
      "10992\n",
      "Get sample from node91\n",
      "18791\n",
      "Get sample from node92\n",
      "17582\n",
      "Get sample from node93\n",
      "37572\n",
      "Get sample from node94\n",
      "33952\n",
      "Get sample from node95\n",
      "28377\n",
      "Get sample from node96\n",
      "25572\n",
      "Get sample from node97\n",
      "19582\n",
      "Get sample from node98\n",
      "40787\n",
      "Get sample from node99\n",
      "17789\n",
      "Get sample from node100\n",
      "10972\n",
      "Get sample from node101\n",
      "18145\n",
      "Get sample from node102\n",
      "11581\n",
      "Get sample from node103\n",
      "27372\n",
      "Get sample from node104\n",
      "19579\n",
      "Get sample from node105\n",
      "21798\n",
      "Get sample from node106\n",
      "10372\n",
      "Get sample from node107\n",
      "33581\n",
      "Get sample from node108\n",
      "32773\n",
      "Get sample from node109\n",
      "44762\n",
      "Get sample from node110\n",
      "24572\n",
      "Get sample from node111\n",
      "22182\n",
      "Get sample from node112\n",
      "42142\n",
      "Get sample from node113\n",
      "7222\n",
      "Get sample from node114\n",
      "16982\n",
      "Get sample from node115\n",
      "27162\n",
      "Get sample from node116\n",
      "39552\n",
      "Get sample from node117\n",
      "29572\n",
      "Get sample from node118\n",
      "25551\n",
      "Get sample from node119\n",
      "17181\n",
      "Get sample from node120\n",
      "35562\n",
      "Get sample from node121\n",
      "42951\n",
      "Get sample from node122\n",
      "35572\n",
      "Get sample from node123\n",
      "19182\n",
      "Get sample from node124\n",
      "31162\n",
      "Get sample from node125\n",
      "37166\n",
      "Get sample from node126\n",
      "34162\n",
      "Get sample from node127\n",
      "43942\n",
      "Get sample from node128\n",
      "24777\n",
      "Get sample from node129\n",
      "42973\n",
      "Get sample from node130\n",
      "27163\n",
      "Get sample from node131\n",
      "26172\n",
      "Get sample from node132\n",
      "23182\n",
      "Get sample from node133\n",
      "19972\n",
      "Get sample from node134\n",
      "18581\n",
      "Get sample from node135\n",
      "28572\n",
      "Get sample from node136\n",
      "27380\n",
      "Get sample from node137\n",
      "32579\n",
      "Get sample from node138\n",
      "14775\n",
      "Get sample from node139\n",
      "11182\n",
      "Get sample from node140\n",
      "30571\n",
      "Get sample from node141\n",
      "42772\n",
      "Get sample from node142\n",
      "13982\n",
      "Get sample from node143\n",
      "18162\n",
      "Get sample from node144\n",
      "38782\n",
      "Get sample from node145\n",
      "12172\n",
      "Get sample from node146\n",
      "17151\n",
      "Get sample from node147\n",
      "21982\n",
      "Get sample from node148\n",
      "22172\n",
      "Get sample from node149\n",
      "28571\n",
      "Get sample from node150\n",
      "25361\n",
      "Get sample from node151\n",
      "19371\n",
      "Get sample from node152\n",
      "13181\n",
      "Get sample from node153\n",
      "13171\n",
      "Get sample from node154\n",
      "7181\n",
      "Get sample from node155\n",
      "991\n",
      "Sampling is completed, sample length:  4059538\n"
     ]
    }
   ],
   "source": [
    "node_num = 155\n",
    "num_RSSI_sample = 10\n",
    "\n",
    "data_set = np.empty((num_RSSI_sample))\n",
    "channel_label_set = np.empty((1))\n",
    "TA_label_set = np.empty((1))\n",
    "\n",
    "for node_index in range(1,node_num+1):\n",
    "    print('Get sample from node'+str(node_index))\n",
    "    file_name = 'data/node-'+str(node_index)+'.txt'\n",
    "    previous_cellId = -1\n",
    "    cell_sample = list()\n",
    "    cell_sample_set = list()\n",
    "    \n",
    "    with open(file_name, 'r') as f:\n",
    "        while 1:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            parsed_line =line.split(' ')\n",
    "            \n",
    "            rssi = parsed_line[9]\n",
    "            channel_state = parsed_line[8]\n",
    "            current_cellId = parsed_line[10]\n",
    "            TA_region = parsed_line[11]\n",
    "            \n",
    "            if previous_cellId == current_cellId:\n",
    "                cell_sample.append([rssi, channel_state, TA_region])\n",
    "            else:\n",
    "                previous_cellId = current_cellId\n",
    "                if cell_sample:\n",
    "                    cell_sample_set.append(cell_sample)\n",
    "                    cell_sample = list()\n",
    "        \n",
    "        cell_sample_set.append(cell_sample)\n",
    "        sample_length = 0\n",
    "        for i in range(len(cell_sample_set)):\n",
    "            sample_length += len(cell_sample_set[i])-num_RSSI_sample+1\n",
    "        \n",
    "        print(sample_length)\n",
    "        \n",
    "        temp_data_set = np.zeros((sample_length,num_RSSI_sample))\n",
    "        temp_channel_label_set = np.zeros((sample_length,1))\n",
    "        temp_TA_label_set = np.zeros((sample_length,1))\n",
    "        \n",
    "        for i in range(len(cell_sample_set)):\n",
    "            cell_sample = cell_sample_set[i]\n",
    "            cell_sample = np.array(cell_sample)\n",
    "            for j in range(len(cell_sample_set[i])-num_RSSI_sample+1):\n",
    "                temp_data_set[j] = cell_sample[j:j+num_RSSI_sample,0]\n",
    "                temp_channel_label_set[j] = cell_sample[j+num_RSSI_sample-1,1]\n",
    "                temp_TA_label_set[j] = cell_sample[j+num_RSSI_sample-1,2] \n",
    "              \n",
    "    if node_index == 1:\n",
    "        data_set = temp_data_set\n",
    "        channel_label_set = temp_channel_label_set\n",
    "        TA_label_set = temp_TA_label_set\n",
    "    else:\n",
    "        data_set = np.append(data_set,temp_data_set,axis = 0)\n",
    "        channel_label_set = np.append(channel_label_set,temp_channel_label_set,axis = 0)\n",
    "        TA_label_set = np.append(TA_label_set,temp_TA_label_set,axis=0)\n",
    "    \n",
    "print('Sampling is completed, sample length: ',data_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meme\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print('me'+'me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_length = data_set.shape[0]\n",
    "valid_num = 100000\n",
    "test_num = 100000\n",
    "\n",
    "#get test set\n",
    "test_index = random.sample(range(0,data_length-test_num),test_num)\n",
    "test_data = data_set[test_index]\n",
    "test_channel_label = channel_label_set[test_index]\n",
    "test_TA_label = TA_label_set[test_index]\n",
    "\n",
    "#get training set/validation set\n",
    "train_data_set = np.delete(data_set,test_index,axis=0)\n",
    "train_channel_label_set = np.delete(channel_label_set,test_index,axis=0)\n",
    "train_TA_label_set = np.delete(TA_label_set,test_index,axis=0)\n",
    "data_length = train_data_set.shape[0]\n",
    "\n",
    "valid_index = random.sample(range(0,data_length),valid_num)\n",
    "\n",
    "training_data = train_data_set\n",
    "training_channel_label = train_channel_label_set\n",
    "training_TA_label = train_TA_label_set\n",
    "\n",
    "valid_data = train_data_set[valid_index]\n",
    "valid_channel_label = train_channel_label_set[valid_index]\n",
    "valid_TA_label = train_TA_label_set[valid_index]\n",
    "\n",
    "save_data ={\n",
    "    'training_data':training_data,\n",
    "    'training_channel_label':training_channel_label,\n",
    "    'training_TA_label':training_TA_label,\n",
    "    \n",
    "    'valid_data':valid_data,\n",
    "    'valid_channel_label':valid_channel_label,\n",
    "    'valid_TA_label':valid_TA_label,\n",
    "    \n",
    "    'test_data':test_data,\n",
    "    'test_channel_label':test_channel_label,\n",
    "    'test_TA_label':test_TA_label\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name of the data path\n",
    "data_path = 'data/save_data'+str(num_RSSI_sample)+'.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "with open(data_path,'wb') as f:\n",
    "    pickle.dump(save_data,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (3959538, 10) (3959538, 1)\n",
      "Validation set (100000, 10) (100000, 1)\n",
      "Test set (100000, 10) (100000, 1)\n"
     ]
    }
   ],
   "source": [
    "# restore data\n",
    "with open(data_path,'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    \n",
    "    train_dataset = save['training_data']\n",
    "    train_channel_labels = save['training_channel_label']\n",
    "    train_labels = save['training_TA_label']\n",
    "    \n",
    "    valid_dataset = save['valid_data']\n",
    "    valid_channel_labels = save['valid_channel_label']\n",
    "    valid_labels = save['valid_TA_label']\n",
    "    \n",
    "    test_dataset = save['test_data']\n",
    "    test_channel_labels = save['test_channel_label']\n",
    "    test_labels = save['test_TA_label']\n",
    "    \n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gslee/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (3959538, 10) (3959538, 4)\n",
      "Validation set (100000, 10) (100000, 4)\n",
      "Test set (100000, 10) (100000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gslee/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/gslee/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, num_RSSI_sample)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    enc.fit(labels)\n",
    "    labels = enc.transform(labels).toarray()\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = num_RSSI_sample\n",
    "num_labels = 4\n",
    "\n",
    "layer1_output_num = 200\n",
    "layer2_output_num = 200\n",
    "layer3_output_num = 1024\n",
    "layer4_output_num = 512\n",
    "layer5_output_num = 256\n",
    "num_steps = 1000\n",
    "batch_size = 100000\n",
    "\n",
    "graph_gs=tf.Graph()\n",
    "with graph_gs.as_default():\n",
    "    tf_dataset_gs=tf.placeholder(tf.float32, shape=(None, sample_size))\n",
    "    tf_labels_gs=tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    keep_prob = tf.placeholder(tf.float32, shape=(None))\n",
    "    \n",
    "    lambda_reg = 0.1\n",
    "        \n",
    "    #Regularization\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=lambda_reg)\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer(dtype=tf.float32)\n",
    "    #neural network consists of two lines\n",
    "    dense1 = tf.layers.dense(tf_dataset_gs, layer1_output_num, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    dense1 = tf.nn.dropout(dense1, keep_prob = keep_prob)\n",
    "    dense2 = tf.layers.dense(dense1,layer2_output_num, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    dense2 = tf.nn.dropout(dense2, keep_prob = keep_prob)\n",
    "    dense3 = tf.layers.dense(dense2 ,layer3_output_num, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    dense3 = tf.nn.dropout(dense3, keep_prob = keep_prob)\n",
    "    dense4 = tf.layers.dense(dense3 ,layer4_output_num, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    dense4 = tf.nn.dropout(dense4, keep_prob = keep_prob)\n",
    "    dense5 = tf.layers.dense(dense4 ,layer4_output_num, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    dense5 = tf.nn.dropout(dense5, keep_prob = keep_prob)\n",
    "    logits_gs = tf.layers.dense(dense2, num_labels, activation=None)\n",
    "    \n",
    "    #Loss\n",
    "    loss_gs = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_labels_gs, logits=logits_gs))\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.005\n",
    "    \n",
    "    decay_num =  50*(int)(train_labels.shape[0]/batch_size)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, decay_num, 1, staircase=True)\n",
    "    # Optimizer\n",
    "    optimizer_gs = tf.train.AdamOptimizer(learning_rate).minimize(loss_gs, global_step=global_step)\n",
    "    \n",
    "    #Predictions for the training\n",
    "    prediction_gs = tf.nn.softmax(logits_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 307.021542\n",
      "Validation accuracy: 59.9%\n",
      "Minibatch loss at step 1: 27.795916\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 2: 25.987112\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 3: 25.551105\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 4: 25.283938\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 5: 25.073948\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 6: 24.885525\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 7: 24.700439\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 8: 24.530154\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 9: 24.329343\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 10: 24.120857\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 11: 23.954716\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 12: 23.899897\n",
      "Validation accuracy: 64.9%\n",
      "Minibatch loss at step 13: 24.052851\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 14: 23.462222\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 15: 22.958670\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 16: 23.331211\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 17: 23.064732\n",
      "Validation accuracy: 72.1%\n",
      "Minibatch loss at step 18: 23.119875\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 19: 23.027331\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 20: 21.701042\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 21: 23.892204\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 22: 21.226601\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 23: 22.680801\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 24: 20.662621\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 25: 20.576874\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 26: 20.332588\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 27: 20.058227\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 28: 19.818040\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 29: 19.398042\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 30: 27.274354\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 31: 23.039338\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 32: 22.093443\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 33: 22.073034\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 34: 20.696416\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 35: 24.223439\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 36: 21.572180\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 37: 21.024011\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 38: 20.310451\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 39: 19.498978\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 40: 18.637633\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 41: 17.978051\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 42: 17.605999\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 43: 22.723438\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 44: 19.123517\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 45: 18.432414\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 46: 17.748208\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 47: 17.078634\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 48: 22.044692\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 49: 17.860634\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 50: 17.001848\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 51: 16.168497\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 52: 15.675588\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 53: 15.781950\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 54: 15.706400\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 55: 15.619098\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 56: 14.836515\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 57: 14.948702\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 58: 14.584107\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 59: 14.487622\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 60: 14.082165\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 61: 14.591566\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 62: 13.574851\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 63: 14.110778\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 64: 13.761889\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 65: 13.324920\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 66: 14.067766\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 67: 12.889311\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 68: 13.695553\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 69: 12.829877\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 70: 13.522341\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 71: 12.659895\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 72: 12.941519\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 73: 12.921173\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 74: 13.381211\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 75: 12.383826\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 76: 12.927107\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 77: 12.715206\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 78: 12.058026\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 79: 13.262631\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 80: 11.931079\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 81: 12.761784\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 82: 12.091071\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 83: 12.619767\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 84: 12.034703\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 85: 12.506982\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 86: 11.945419\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 87: 12.296745\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 88: 12.244085\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 89: 11.830578\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 90: 12.557406\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 91: 11.641475\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 92: 12.612682\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 93: 11.619252\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 94: 12.281366\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 95: 11.881864\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 96: 11.754874\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 97: 12.200806\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 98: 11.732441\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 99: 11.943511\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 100: 11.886040\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 101: 12.016651\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 102: 11.701809\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 103: 11.445344\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 104: 12.307205\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 105: 11.496890\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 106: 12.139332\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 107: 11.428772\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 108: 12.187176\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 109: 11.442445\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 110: 11.590159\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 111: 11.634279\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 112: 12.079682\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 113: 11.358836\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 114: 11.985396\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 115: 11.473950\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 116: 11.604856\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 117: 12.091810\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 118: 11.296515\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 119: 11.390144\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 120: 11.650447\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 121: 11.959270\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 122: 11.500635\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 123: 11.439125\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 124: 11.566767\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 125: 11.330398\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 126: 11.853245\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 127: 11.471415\n",
      "Validation accuracy: 86.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 128: 11.469527\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 129: 11.723852\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 130: 11.379695\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 131: 11.849954\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 132: 11.484133\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 133: 11.190518\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 134: 11.427199\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 135: 11.766766\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 136: 11.241841\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 137: 11.639317\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 138: 11.570413\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 139: 11.343943\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 140: 11.469617\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 141: 11.260540\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 142: 11.704328\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 143: 11.183404\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 144: 11.942218\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 145: 11.270349\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 146: 11.288748\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 147: 11.360693\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 148: 11.707032\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 149: 11.363968\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 150: 11.466591\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 151: 11.260809\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 152: 11.607428\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 153: 11.180481\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 154: 11.204549\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 155: 11.898555\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 156: 11.177575\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 157: 11.107791\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 158: 11.408607\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 159: 11.544010\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 160: 11.427458\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 161: 11.299881\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 162: 11.384286\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 163: 11.469305\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 164: 11.434076\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 165: 11.323012\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 166: 11.234227\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 167: 11.392955\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 168: 11.449327\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 169: 11.343800\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 170: 11.297603\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 171: 11.410214\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 172: 11.256846\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 173: 11.279428\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 174: 11.220200\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 175: 11.396017\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 176: 11.336338\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 177: 11.418575\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 178: 11.305666\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 179: 11.458976\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 180: 11.172209\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 181: 11.310941\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 182: 11.549014\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 183: 11.276622\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 184: 11.165738\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 185: 11.223138\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 186: 11.482335\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 187: 11.083340\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 188: 11.297120\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 189: 11.267106\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 190: 11.466078\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 191: 11.154489\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 192: 11.303273\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 193: 11.283826\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 194: 11.248486\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 195: 11.367754\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 196: 11.166792\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 197: 11.419181\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 198: 11.007498\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 199: 11.586809\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 200: 11.339260\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 201: 11.211038\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 202: 11.094454\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 203: 11.378627\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 204: 11.226109\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 205: 11.411972\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 206: 11.147475\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 207: 11.109548\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 208: 11.335080\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 209: 11.012484\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 210: 11.261605\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 211: 11.678331\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 212: 11.096612\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 213: 11.028421\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 214: 11.284145\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 215: 11.342969\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 216: 11.108712\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 217: 11.216058\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 218: 11.142397\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 219: 11.181834\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 220: 11.328152\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 221: 11.308309\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 222: 11.066155\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 223: 11.306528\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 224: 11.218474\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 225: 11.056291\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 226: 11.169664\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 227: 11.276654\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 228: 11.332412\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 229: 11.032318\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 230: 11.089552\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 231: 11.244735\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 232: 11.445758\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 233: 10.991568\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 234: 11.007501\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 235: 11.438769\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 236: 11.028742\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 237: 11.231722\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 238: 11.254300\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 239: 11.164828\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 240: 11.076153\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 241: 11.070695\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 242: 11.370060\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 243: 11.228146\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 244: 10.991951\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 245: 11.084278\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 246: 11.437719\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 247: 10.928128\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 248: 11.145707\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 249: 11.198134\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 250: 11.194280\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 251: 11.074907\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 252: 11.107183\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 253: 11.156957\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 254: 11.118604\n",
      "Validation accuracy: 86.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 255: 11.182338\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 256: 11.308809\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 257: 11.061762\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 258: 11.140257\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 259: 11.312326\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 260: 11.064257\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 261: 11.047774\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 262: 11.214629\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 263: 11.099786\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 264: 11.209942\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 265: 11.136719\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 266: 11.004724\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 267: 11.255009\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 268: 11.094289\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 269: 11.056015\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 270: 11.268712\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 271: 11.014937\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 272: 11.138998\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 273: 11.249384\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 274: 10.930207\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 275: 11.246387\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 276: 11.057042\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 277: 11.020270\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 278: 11.111056\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 279: 11.231989\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 280: 11.157334\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 281: 10.877436\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 282: 11.296297\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 283: 11.085979\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 284: 11.092501\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 285: 11.165509\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 286: 10.977939\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 287: 11.185221\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 288: 10.962420\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 289: 11.267902\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 290: 10.962703\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 291: 11.143318\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 292: 11.158632\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 293: 11.113336\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 294: 11.068555\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 295: 11.007539\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 296: 11.130859\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 297: 11.172101\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 298: 11.005579\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 299: 11.260841\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 300: 11.133347\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 301: 10.968339\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 302: 11.084240\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 303: 11.116334\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 304: 11.255783\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 305: 11.022717\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 306: 10.962032\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 307: 11.263534\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 308: 11.026064\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 309: 11.000554\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 310: 11.097280\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 311: 11.025479\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 312: 11.076813\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 313: 11.178453\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 314: 11.057407\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 315: 11.061874\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 316: 11.076210\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 317: 11.026312\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 318: 11.135589\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 319: 11.158650\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 320: 11.036746\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 321: 11.161999\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 322: 10.968721\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 323: 11.158705\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 324: 11.071165\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 325: 11.219680\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 326: 10.952972\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 327: 11.125071\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 328: 11.105682\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 329: 10.971845\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 330: 11.228037\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 331: 11.041925\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 332: 11.006362\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 333: 11.016847\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 334: 10.906579\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 335: 11.302090\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 336: 10.999821\n",
      "Validation accuracy: 84.8%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.equal(np.argmax(predictions, 1), np.argmax(labels, 1)))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "batch_num = (int)(train_labels.shape[0]/batch_size)\n",
    "\n",
    "with tf.Session(graph=graph_gs, config=conf) as session_gs:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        random = np.random.permutation(train_labels.shape[0])\n",
    "        loss = 0\n",
    "        for b in range(batch_num):\n",
    "            batch_data = train_dataset[random[b*batch_size:(b+1)*batch_size]]\n",
    "            batch_labels = train_labels[random[b*batch_size:(b+1)*batch_size]].astype(float)\n",
    "            feed_dict_gs = {tf_dataset_gs: batch_data, tf_labels_gs: batch_labels, keep_prob:1}\n",
    "            _, l_gs, predictions_l = session_gs.run([optimizer_gs, loss_gs, prediction_gs], feed_dict=feed_dict_gs)\n",
    "            loss += l_gs\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, loss))\n",
    "        feed_dict_val_gs = {tf_dataset_gs: valid_dataset}\n",
    "        valid_prediction_gs = session_gs.run(prediction_gs, feed_dict={tf_dataset_gs: valid_dataset, tf_labels_gs: valid_labels, keep_prob:1})\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction_gs, valid_labels))\n",
    "        \n",
    "    feed_dict_test_gs = {tf_dataset_gs: test_dataset, keep_prob:1.0}\n",
    "    test_prediction_gs = session_gs.run(prediction_gs, feed_dict=feed_dict_test_gs)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction_gs, test_labels))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session_gs, \"./model_checkpoints/centralized_sample\"+str(sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 52.5%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_gs, config=conf) as session_gs:\n",
    "    tf.global_variables_initializer().run()\n",
    "    feed_dict_test_gs = {tf_dataset_gs: test_dataset, keep_prob:1}\n",
    "    test_prediction_gs = session_gs.run(prediction_gs, feed_dict=feed_dict_test_gs)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction_gs, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
