{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN approach for EsTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Goodsol Lee, NETLAB, Seoul National University\n",
    "        Junseok Kim, NETLAB, Seoul National University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os\n",
    "\n",
    "#configuration for gpu usage\n",
    "conf = tf.ConfigProto()\n",
    "# you can modify below as you want\n",
    "#conf.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "#conf.gpu_options.allow_growth = True\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Sample training data, validation data, test data from raw data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get sample from node1\n",
      "25703\n",
      "Get sample from node2\n",
      "15563\n",
      "Get sample from node3\n",
      "25310\n",
      "Get sample from node4\n",
      "18562\n",
      "Get sample from node5\n",
      "26542\n",
      "Get sample from node6\n",
      "21153\n",
      "Get sample from node7\n",
      "30533\n",
      "Get sample from node8\n",
      "52925\n",
      "Get sample from node9\n",
      "26561\n",
      "Get sample from node10\n",
      "21729\n",
      "Get sample from node11\n",
      "21502\n",
      "Get sample from node12\n",
      "16722\n",
      "Get sample from node13\n",
      "33341\n",
      "Get sample from node14\n",
      "23348\n",
      "Get sample from node15\n",
      "52122\n",
      "Get sample from node16\n",
      "34743\n",
      "Get sample from node17\n",
      "26342\n",
      "Get sample from node18\n",
      "29130\n",
      "Get sample from node19\n",
      "22342\n",
      "Get sample from node20\n",
      "24542\n",
      "Get sample from node21\n",
      "41922\n",
      "Get sample from node22\n",
      "40882\n",
      "Get sample from node23\n",
      "11781\n",
      "Get sample from node24\n",
      "23942\n",
      "Get sample from node25\n",
      "27941\n",
      "Get sample from node26\n",
      "42326\n",
      "Get sample from node27\n",
      "29342\n",
      "Get sample from node28\n",
      "20762\n",
      "Get sample from node29\n",
      "24161\n",
      "Get sample from node30\n",
      "25164\n",
      "Get sample from node31\n",
      "35146\n",
      "Get sample from node32\n",
      "34142\n",
      "Get sample from node33\n",
      "25161\n",
      "Get sample from node34\n",
      "20541\n",
      "Get sample from node35\n",
      "17742\n",
      "Get sample from node36\n",
      "29142\n",
      "Get sample from node37\n",
      "22171\n",
      "Get sample from node38\n",
      "9380\n",
      "Get sample from node39\n",
      "24529\n",
      "Get sample from node40\n",
      "20562\n",
      "Get sample from node41\n",
      "17561\n",
      "Get sample from node42\n",
      "22961\n",
      "Get sample from node43\n",
      "15919\n",
      "Get sample from node44\n",
      "22742\n",
      "Get sample from node45\n",
      "35484\n",
      "Get sample from node46\n",
      "38144\n",
      "Get sample from node47\n",
      "27342\n",
      "Get sample from node48\n",
      "32342\n",
      "Get sample from node49\n",
      "20760\n",
      "Get sample from node50\n",
      "14361\n",
      "Get sample from node51\n",
      "41363\n",
      "Get sample from node52\n",
      "27902\n",
      "Get sample from node53\n",
      "14761\n",
      "Get sample from node54\n",
      "26341\n",
      "Get sample from node55\n",
      "27362\n",
      "Get sample from node56\n",
      "21761\n",
      "Get sample from node57\n",
      "41328\n",
      "Get sample from node58\n",
      "27134\n",
      "Get sample from node59\n",
      "23941\n",
      "Get sample from node60\n",
      "27902\n",
      "Get sample from node61\n",
      "23561\n",
      "Get sample from node62\n",
      "25762\n",
      "Get sample from node63\n",
      "34932\n",
      "Get sample from node64\n",
      "27342\n",
      "Get sample from node65\n",
      "19562\n",
      "Get sample from node66\n",
      "23362\n",
      "Get sample from node67\n",
      "39682\n",
      "Get sample from node68\n",
      "11382\n",
      "Get sample from node69\n",
      "20925\n",
      "Get sample from node70\n",
      "27541\n",
      "Get sample from node71\n",
      "25142\n",
      "Get sample from node72\n",
      "18542\n",
      "Get sample from node73\n",
      "22142\n",
      "Get sample from node74\n",
      "35922\n",
      "Get sample from node75\n",
      "30522\n",
      "Get sample from node76\n",
      "16161\n",
      "Get sample from node77\n",
      "23761\n",
      "Get sample from node78\n",
      "26122\n",
      "Get sample from node79\n",
      "43722\n",
      "Get sample from node80\n",
      "38142\n",
      "Get sample from node81\n",
      "46882\n",
      "Get sample from node82\n",
      "27961\n",
      "Get sample from node83\n",
      "20538\n",
      "Get sample from node84\n",
      "28967\n",
      "Get sample from node85\n",
      "44730\n",
      "Get sample from node86\n",
      "22561\n",
      "Get sample from node87\n",
      "41545\n",
      "Get sample from node88\n",
      "26929\n",
      "Get sample from node89\n",
      "22882\n",
      "Get sample from node90\n",
      "10982\n",
      "Get sample from node91\n",
      "18781\n",
      "Get sample from node92\n",
      "17562\n",
      "Get sample from node93\n",
      "37542\n",
      "Get sample from node94\n",
      "33902\n",
      "Get sample from node95\n",
      "28347\n",
      "Get sample from node96\n",
      "25542\n",
      "Get sample from node97\n",
      "19562\n",
      "Get sample from node98\n",
      "40767\n",
      "Get sample from node99\n",
      "17769\n",
      "Get sample from node100\n",
      "10942\n",
      "Get sample from node101\n",
      "18105\n",
      "Get sample from node102\n",
      "11561\n",
      "Get sample from node103\n",
      "27342\n",
      "Get sample from node104\n",
      "19549\n",
      "Get sample from node105\n",
      "21768\n",
      "Get sample from node106\n",
      "10342\n",
      "Get sample from node107\n",
      "33561\n",
      "Get sample from node108\n",
      "32733\n",
      "Get sample from node109\n",
      "44722\n",
      "Get sample from node110\n",
      "24542\n",
      "Get sample from node111\n",
      "22162\n",
      "Get sample from node112\n",
      "42082\n",
      "Get sample from node113\n",
      "7202\n",
      "Get sample from node114\n",
      "16962\n",
      "Get sample from node115\n",
      "27122\n",
      "Get sample from node116\n",
      "39502\n",
      "Get sample from node117\n",
      "29542\n",
      "Get sample from node118\n",
      "25501\n",
      "Get sample from node119\n",
      "17161\n",
      "Get sample from node120\n",
      "35522\n",
      "Get sample from node121\n",
      "42901\n",
      "Get sample from node122\n",
      "35542\n",
      "Get sample from node123\n",
      "19162\n",
      "Get sample from node124\n",
      "31122\n",
      "Get sample from node125\n",
      "37126\n",
      "Get sample from node126\n",
      "34122\n",
      "Get sample from node127\n",
      "43882\n",
      "Get sample from node128\n",
      "24757\n",
      "Get sample from node129\n",
      "42943\n",
      "Get sample from node130\n",
      "27123\n",
      "Get sample from node131\n",
      "26142\n",
      "Get sample from node132\n",
      "23162\n",
      "Get sample from node133\n",
      "19942\n",
      "Get sample from node134\n",
      "18561\n",
      "Get sample from node135\n",
      "28542\n",
      "Get sample from node136\n",
      "27350\n",
      "Get sample from node137\n",
      "32549\n",
      "Get sample from node138\n",
      "14755\n",
      "Get sample from node139\n",
      "11162\n",
      "Get sample from node140\n",
      "30541\n",
      "Get sample from node141\n",
      "42742\n",
      "Get sample from node142\n",
      "13962\n",
      "Get sample from node143\n",
      "18122\n",
      "Get sample from node144\n",
      "38752\n",
      "Get sample from node145\n",
      "12142\n",
      "Get sample from node146\n",
      "17101\n",
      "Get sample from node147\n",
      "21962\n",
      "Get sample from node148\n",
      "22142\n",
      "Get sample from node149\n",
      "28541\n",
      "Get sample from node150\n",
      "25321\n",
      "Get sample from node151\n",
      "19341\n",
      "Get sample from node152\n",
      "13161\n",
      "Get sample from node153\n",
      "13141\n",
      "Get sample from node154\n",
      "7161\n",
      "Get sample from node155\n",
      "981\n",
      "Sampling is completed, sample length:  4054728\n"
     ]
    }
   ],
   "source": [
    "node_num = 155\n",
    "num_RSSI_sample = 20\n",
    "scs = 30\n",
    "\n",
    "data_set = np.empty((num_RSSI_sample))\n",
    "channel_label_set = np.empty((1))\n",
    "TA_label_set = np.empty((1))\n",
    "\n",
    "for node_index in range(1,node_num+1):\n",
    "    print('Get sample from node'+str(node_index))\n",
    "    file_name = 'data-'+str(scs)+'/node-'+str(node_index)+'.txt'\n",
    "    previous_cellId = -1\n",
    "    cell_sample = list()\n",
    "    cell_sample_set = list()\n",
    "    \n",
    "    with open(file_name, 'r') as f:\n",
    "        while 1:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            parsed_line =line.split(' ')\n",
    "            \n",
    "            rssi = parsed_line[9]\n",
    "            channel_state = parsed_line[8]\n",
    "            current_cellId = parsed_line[10]\n",
    "            TA_region = parsed_line[11]\n",
    "            \n",
    "            if previous_cellId == current_cellId:\n",
    "                cell_sample.append([rssi, channel_state, TA_region])\n",
    "            else:\n",
    "                previous_cellId = current_cellId\n",
    "                if cell_sample:\n",
    "                    cell_sample_set.append(cell_sample)\n",
    "                    cell_sample = list()\n",
    "        \n",
    "        cell_sample_set.append(cell_sample)\n",
    "        sample_length = 0\n",
    "        for i in range(len(cell_sample_set)):\n",
    "            sample_length += len(cell_sample_set[i])-num_RSSI_sample+1\n",
    "        \n",
    "        print(sample_length)\n",
    "        \n",
    "        temp_data_set = np.zeros((sample_length,num_RSSI_sample))\n",
    "        temp_channel_label_set = np.zeros((sample_length,1))\n",
    "        temp_TA_label_set = np.zeros((sample_length,1))\n",
    "        \n",
    "        iter_start = 0\n",
    "        iter_end = 0\n",
    "        for i in range(len(cell_sample_set)):\n",
    "            cell_sample = cell_sample_set[i]\n",
    "            cell_sample = np.array(cell_sample)\n",
    "            iter_start = iter_end\n",
    "            iter_end = iter_end + len(cell_sample_set[i])-num_RSSI_sample+1\n",
    "            for j in range(len(cell_sample_set[i])-num_RSSI_sample+1):\n",
    "                temp_data_set[iter_start+j] = cell_sample[j:j+num_RSSI_sample,0]\n",
    "                temp_channel_label_set[iter_start+j] = cell_sample[j+num_RSSI_sample-1,1]\n",
    "                temp_TA_label_set[iter_start+j] = cell_sample[j+num_RSSI_sample-1,2] \n",
    "              \n",
    "    if node_index == 1:\n",
    "        data_set = temp_data_set\n",
    "        channel_label_set = temp_channel_label_set\n",
    "        TA_label_set = temp_TA_label_set\n",
    "    else:\n",
    "        data_set = np.append(data_set,temp_data_set,axis = 0)\n",
    "        channel_label_set = np.append(channel_label_set,temp_channel_label_set,axis = 0)\n",
    "        TA_label_set = np.append(TA_label_set,temp_TA_label_set,axis=0)\n",
    "    \n",
    "print('Sampling is completed, sample length: ',data_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "7.0\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "zeroidx = np.where(data_set==0)\n",
    "\n",
    "print(zeroidx)\n",
    "\n",
    "print(np.max(TA_label_set))\n",
    "\n",
    "for i in range(1):\n",
    "    print('-----------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_length = data_set.shape[0]\n",
    "valid_num = 100000\n",
    "test_num = 100000\n",
    "\n",
    "#get test set\n",
    "test_index = random.sample(range(0,data_length-test_num),test_num)\n",
    "test_data = data_set[test_index]\n",
    "test_channel_label = channel_label_set[test_index]\n",
    "test_TA_label = TA_label_set[test_index]\n",
    "\n",
    "#get training set/validation set\n",
    "train_data_set = np.delete(data_set,test_index,axis=0)\n",
    "train_channel_label_set = np.delete(channel_label_set,test_index,axis=0)\n",
    "train_TA_label_set = np.delete(TA_label_set,test_index,axis=0)\n",
    "data_length = train_data_set.shape[0]\n",
    "\n",
    "valid_index = random.sample(range(0,data_length),valid_num)\n",
    "\n",
    "training_data = train_data_set\n",
    "training_channel_label = train_channel_label_set\n",
    "training_TA_label = train_TA_label_set\n",
    "\n",
    "valid_data = train_data_set[valid_index]\n",
    "valid_channel_label = train_channel_label_set[valid_index]\n",
    "valid_TA_label = train_TA_label_set[valid_index]\n",
    "\n",
    "save_data ={\n",
    "    'training_data':training_data,\n",
    "    'training_channel_label':training_channel_label,\n",
    "    'training_TA_label':training_TA_label,\n",
    "    \n",
    "    'valid_data':valid_data,\n",
    "    'valid_channel_label':valid_channel_label,\n",
    "    'valid_TA_label':valid_TA_label,\n",
    "    \n",
    "    'test_data':test_data,\n",
    "    'test_channel_label':test_channel_label,\n",
    "    'test_TA_label':test_TA_label\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name of the data path\n",
    "data_path = 'data-'+str(scs)+'/save_data'+str(num_RSSI_sample)+'.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "with open(data_path,'wb') as f:\n",
    "    pickle.dump(save_data,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (3954728, 20) (3954728, 1)\n",
      "Validation set (100000, 20) (100000, 1)\n",
      "Test set (100000, 20) (100000, 1)\n"
     ]
    }
   ],
   "source": [
    "# restore data\n",
    "with open(data_path,'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    \n",
    "    train_dataset = save['training_data']\n",
    "    train_channel_labels = save['training_channel_label']\n",
    "    train_labels = save['training_TA_label']\n",
    "    \n",
    "    valid_dataset = save['valid_data']\n",
    "    valid_channel_labels = save['valid_channel_label']\n",
    "    valid_labels = save['valid_TA_label']\n",
    "    \n",
    "    test_dataset = save['test_data']\n",
    "    test_channel_labels = save['test_channel_label']\n",
    "    test_labels = save['test_TA_label']\n",
    "    \n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gslee/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (3954728, 20) (3954728, 8)\n",
      "Validation set (100000, 20) (100000, 8)\n",
      "Test set (100000, 20) (100000, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gslee/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/gslee/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, num_RSSI_sample)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    enc.fit(labels)\n",
    "    labels = enc.transform(labels).toarray()\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = num_RSSI_sample\n",
    "num_labels = np.max(TA_label_set)+1\n",
    "\n",
    "layer1_output_num = 200\n",
    "layer2_output_num = 200\n",
    "layer3_output_num = 200\n",
    "layer4_output_num = 200\n",
    "layer5_output_num = 200\n",
    "num_steps = 100\n",
    "batch_size = 1024\n",
    "\n",
    "graph_gs=tf.Graph()\n",
    "with graph_gs.as_default():\n",
    "    tf_dataset_gs=tf.placeholder(tf.float32, shape=(None, sample_size))\n",
    "    tf_labels_gs=tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    keep_prob = tf.placeholder(tf.float32, shape=(None))\n",
    "    is_train = tf.placeholder(tf.bool)\n",
    "    \n",
    "    lambda_reg = 0.000000000001\n",
    "        \n",
    "    #Regularization\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=lambda_reg)\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer(dtype=tf.float32)\n",
    "    #neural network consists of two lines\n",
    "    dense1 = tf.layers.dense(tf_dataset_gs, layer1_output_num, kernel_regularizer=regularizer)\n",
    "    dense1 = tf.nn.dropout(dense1, keep_prob = keep_prob)\n",
    "    dense1 = tf.layers.batch_normalization(dense1, training=is_train)\n",
    "    \n",
    "    dense2 = tf.layers.dense(dense1,layer2_output_num, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    dense2 = tf.nn.dropout(dense2, keep_prob = keep_prob)\n",
    "    dense2 = tf.layers.batch_normalization(dense2, training=is_train)\n",
    "    \n",
    "    dense3 = tf.layers.dense(dense2 ,layer3_output_num, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    dense3 = tf.nn.dropout(dense3, keep_prob = keep_prob)\n",
    "    dense3 = tf.layers.batch_normalization(dense3, training=is_train)\n",
    "    \n",
    "    dense4 = tf.layers.dense(dense3 ,layer4_output_num, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    dense4 = tf.nn.dropout(dense4, keep_prob = keep_prob)\n",
    "    dense4 = tf.layers.batch_normalization(dense4, training=is_train)\n",
    "    \n",
    "    dense5 = tf.layers.dense(dense4 ,layer4_output_num, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    dense5 = tf.nn.dropout(dense5, keep_prob = keep_prob)\n",
    "    dense5 = tf.layers.batch_normalization(dense4, training=is_train)\n",
    "    logits_gs = tf.layers.dense(dense3,num_labels, activation=None)\n",
    "    \n",
    "    #Loss\n",
    "    loss_gs = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_labels_gs, logits=logits_gs))\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.01\n",
    "    decay_num =  50*(int)(train_labels.shape[0]/batch_size)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, decay_num, 1, staircase=True)\n",
    "    # Optimizer\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizer_gs = tf.train.AdamOptimizer(learning_rate).minimize(loss_gs, global_step=global_step)\n",
    "    \n",
    "    #Predictions for the training\n",
    "    prediction_gs = tf.nn.softmax(logits_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3270.693318\n",
      "Validation accuracy: 95.2%\n",
      "Minibatch loss at step 1: 3227.908419\n",
      "Validation accuracy: 96.3%\n",
      "Minibatch loss at step 2: 3215.467755\n",
      "Validation accuracy: 98.0%\n",
      "Minibatch loss at step 3: 3207.573943\n",
      "Validation accuracy: 98.3%\n",
      "Minibatch loss at step 4: 3200.917734\n",
      "Validation accuracy: 98.3%\n",
      "Minibatch loss at step 5: 3189.303746\n",
      "Validation accuracy: 98.7%\n",
      "Minibatch loss at step 6: 3176.897088\n",
      "Validation accuracy: 98.7%\n",
      "Minibatch loss at step 7: 3172.828338\n",
      "Validation accuracy: 98.7%\n",
      "Minibatch loss at step 8: 3168.722084\n",
      "Validation accuracy: 98.8%\n",
      "Minibatch loss at step 9: 3162.638945\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 10: 3158.684735\n",
      "Validation accuracy: 98.8%\n",
      "Minibatch loss at step 11: 3155.552126\n",
      "Validation accuracy: 98.8%\n",
      "Minibatch loss at step 12: 3154.032474\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 13: 3152.083965\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 14: 3151.096395\n",
      "Validation accuracy: 98.7%\n",
      "Minibatch loss at step 15: 3148.862501\n",
      "Validation accuracy: 98.7%\n",
      "Minibatch loss at step 16: 3148.274192\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 17: 3148.526662\n",
      "Validation accuracy: 98.7%\n",
      "Minibatch loss at step 18: 3146.466982\n",
      "Validation accuracy: 98.8%\n",
      "Minibatch loss at step 19: 3147.318869\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 20: 3144.975304\n",
      "Validation accuracy: 98.8%\n",
      "Minibatch loss at step 21: 3143.610188\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 22: 3143.149033\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 23: 3141.354326\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 24: 3140.539039\n",
      "Validation accuracy: 98.8%\n",
      "Minibatch loss at step 25: 3140.093863\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 26: 3139.262822\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 27: 3139.002465\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 28: 3138.331417\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 29: 3138.140482\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 30: 3137.236509\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 31: 3136.849707\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 32: 3135.911180\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 33: 3135.600063\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 34: 3134.728517\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 35: 3135.172358\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 36: 3133.997386\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 37: 3133.835361\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 38: 3133.388759\n",
      "Validation accuracy: 98.8%\n",
      "Minibatch loss at step 39: 3133.394123\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 40: 3133.960163\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 41: 3133.497480\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 42: 3132.894937\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 43: 3133.402995\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 44: 3132.304470\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 45: 3131.138915\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 46: 3133.058775\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 47: 3131.302013\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 48: 3131.380017\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 49: 3132.347307\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 50: 3130.327121\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 51: 3131.870278\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 52: 3129.993927\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 53: 3129.736968\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 54: 3130.470036\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 55: 3130.709797\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 56: 3129.571821\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 57: 3129.757638\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 58: 3129.547245\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 59: 3129.743034\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 60: 3128.734355\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 61: 3129.081317\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 62: 3128.412688\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 63: 3128.571321\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 64: 3128.921301\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 65: 3128.528810\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 66: 3129.058292\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 67: 3128.286132\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 68: 3128.528024\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 69: 3127.732147\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 70: 3127.440151\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 71: 3127.641859\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 72: 3128.377147\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 73: 3127.275441\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 74: 3127.857794\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 75: 3127.160291\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 76: 3127.600971\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 77: 3127.478922\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 78: 3126.635500\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 79: 3126.839482\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 80: 3126.928462\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 81: 3127.214143\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 82: 3127.247283\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 83: 3127.260666\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 84: 3127.158502\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 85: 3127.322333\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 86: 3126.578147\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 87: 3126.208019\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 88: 3126.585460\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 89: 3126.123397\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 90: 3126.104975\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 91: 3125.588327\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 92: 3125.572586\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 93: 3125.239148\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 94: 3125.167597\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 95: 3124.987280\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 96: 3123.157579\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 97: 3118.973447\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 98: 3118.400398\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 99: 3118.230331\n",
      "Validation accuracy: 99.0%\n",
      "Test accuracy: 99.0%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    #original \n",
    "    #return (100.0 * np.sum(np.equal(np.argmax(predictions, 1), np.argmax(labels, 1)))\n",
    "    #      / predictions.shape[0])\n",
    "    #timing error limit requirement\n",
    "    return (100.0 * np.sum(np.equal(np.argmax(predictions, 1), np.argmax(labels, 1))+\n",
    "                           np.equal(np.argmax(predictions, 1), np.argmax(labels, 1)+1)+\n",
    "                           np.equal(np.argmax(predictions, 1), np.argmax(labels, 1)-1))/predictions.shape[0])   \n",
    "\n",
    "tf.reset_default_graph()\n",
    "batch_num = (int)(train_labels.shape[0]/batch_size)\n",
    "\n",
    "with tf.Session(graph=graph_gs, config=conf) as session_gs:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        random = np.random.permutation(train_labels.shape[0])\n",
    "        loss = 0\n",
    "        for b in range(batch_num):\n",
    "            batch_data = train_dataset[random[b*batch_size:(b+1)*batch_size]]\n",
    "            batch_labels = train_labels[random[b*batch_size:(b+1)*batch_size]].astype(float)\n",
    "            feed_dict_gs = {tf_dataset_gs: batch_data, tf_labels_gs: batch_labels, keep_prob:1, is_train:True}\n",
    "            _, l_gs, predictions_l = session_gs.run([optimizer_gs, loss_gs, prediction_gs], feed_dict=feed_dict_gs)\n",
    "            loss += l_gs\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, loss))\n",
    "        \n",
    "        temp_acc = 0\n",
    "        valid_batch_num = (int)(valid_labels.shape[0]/1000)\n",
    "        for i in range(valid_batch_num):\n",
    "            valid_prediction_gs = session_gs.run(prediction_gs, feed_dict={tf_dataset_gs: valid_dataset[i*1000:(i+1)*1000], tf_labels_gs: valid_labels[i*1000:(i+1)*1000], keep_prob:1, is_train: False})\n",
    "            temp_acc += accuracy(valid_prediction_gs, valid_labels[i*1000:(i+1)*1000])\n",
    "        print(\"Validation accuracy: %.1f%%\" %(temp_acc/valid_batch_num))\n",
    "        \n",
    "    test_acc = 0\n",
    "    test_batch_num = (int)(test_labels.shape[0]/1000)\n",
    "    for i in range(test_batch_num):\n",
    "        feed_dict_test_gs = {tf_dataset_gs: test_dataset[i*1000:(i+1)*1000], keep_prob:1.0, is_train: False}\n",
    "        test_prediction_gs = session_gs.run(prediction_gs, feed_dict=feed_dict_test_gs)\n",
    "        test_acc += accuracy(test_prediction_gs, test_labels[i*1000:(i+1)*1000])\n",
    "    print(\"Test accuracy: %.1f%%\" % (test_acc/test_batch_num))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session_gs, \"./model_checkpoints/centralized_sample\"+str(sample_size))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
